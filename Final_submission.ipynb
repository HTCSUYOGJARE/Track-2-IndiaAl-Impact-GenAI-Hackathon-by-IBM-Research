{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd67a1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell 1 executed: Setup and configuration complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from transformers import EarlyStoppingCallback, Trainer, TrainingArguments, set_seed\n",
    "\n",
    "from tsfm_public import (\n",
    "    TimeSeriesForecastingPipeline,\n",
    "    TimeSeriesPreprocessor,\n",
    "    TinyTimeMixerForPrediction,\n",
    "    TrackingCallback,\n",
    "    count_parameters,\n",
    "    get_datasets,\n",
    ")\n",
    "\n",
    "# Transformers and TSFM library imports\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from tsfm_public.toolkit.time_series_preprocessor import TimeSeriesPreprocessor\n",
    "from tsfm_public import get_datasets # Corrected import\n",
    "from tsfm_public import TinyTimeMixerForPrediction\n",
    "from tsfm_public import TinyTimeMixerConfig\n",
    "from tsfm_public.toolkit.time_series_preprocessor import prepare_data_splits\n",
    "from tsfm_public.toolkit.visualization import plot_predictions\n",
    "from tsfm_public.toolkit.service_util import save_deployment_package\n",
    "# --- Configuration ---\n",
    "DATA_PATH = Path(\"./\") # Assuming data is in the same directory\n",
    "TRAIN_FILE_PATH =  \"final_train.csv\"\n",
    "TEST_FILE_PATH =  \"final_test.csv\"\n",
    "METADATA_FILE_PATH =  \"metadata.csv\"\n",
    "\n",
    "timestamp_column = \"timestamp\"\n",
    "context_length = 168\n",
    "prediction_length = 24\n",
    "SEED = 79\n",
    "subset_fraction = 1\n",
    "\n",
    "print(\"Cell 1 executed: Setup and configuration complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f989423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Saving row_ids for submission...\n",
      "Found 13453 windows across 91 buildings in train.\n",
      "[Split per building] train=10720, valid=1310, test=1423\n",
      "\n",
      "Cell 2 executed: Data loaded/merged and chronological splits created per building.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2582976 entries, 0 to 2582975\n",
      "Data columns (total 51 columns):\n",
      " #   Column                      Dtype         \n",
      "---  ------                      -----         \n",
      " 0   building_id                 string        \n",
      " 1   window_id                   int64         \n",
      " 2   timestamp                   datetime64[ns]\n",
      " 3   meter_reading               float64       \n",
      " 4   role                        object        \n",
      " 5   region                      object        \n",
      " 6   rooms                       float64       \n",
      " 7   no_of_people                int64         \n",
      " 8   area_in_sqft                float64       \n",
      " 9   inverter                    float64       \n",
      " 10  lights                      int64         \n",
      " 11  ceiling_fans                int64         \n",
      " 12  air_coolers                 int64         \n",
      " 13  air_conditioners            float64       \n",
      " 14  fridge                      int64         \n",
      " 15  tv                          int64         \n",
      " 16  water_heaters               int64         \n",
      " 17  washing_machine             int64         \n",
      " 18  mixer                       int64         \n",
      " 19  iron                        int64         \n",
      " 20  micro_wave                  int64         \n",
      " 21  hour                        int64         \n",
      " 22  dayofweek                   int64         \n",
      " 23  month                       int64         \n",
      " 24  is_weekend                  int64         \n",
      " 25  meter_lag1                  float64       \n",
      " 26  meter_lag24                 float64       \n",
      " 27  meter_lag168                float64       \n",
      " 28  meter_roll_mean_24h         float64       \n",
      " 29  meter_roll_std_24h          float64       \n",
      " 30  meter_roll_mean_7d          float64       \n",
      " 31  area_per_room               float64       \n",
      " 32  people_per_room             float64       \n",
      " 33  people_per_sqft             float64       \n",
      " 34  lights_per_room             float64       \n",
      " 35  fans_per_room               float64       \n",
      " 36  ac_per_room                 float64       \n",
      " 37  fridge_per_person           float64       \n",
      " 38  appliance_density           float64       \n",
      " 39  has_ac                      int64         \n",
      " 40  has_fridge                  int64         \n",
      " 41  has_inverter                int64         \n",
      " 42  electricity_load_score      float64       \n",
      " 43  region_median_area          float64       \n",
      " 44  deviation_from_region_area  float64       \n",
      " 45  bldg_mean                   float64       \n",
      " 46  bldg_std                    float64       \n",
      " 47  bldg_max                    float64       \n",
      " 48  bldg_peak_to_mean_ratio     float64       \n",
      " 49  is_area_missing             int64         \n",
      " 50  is_inverter_missing         int64         \n",
      "dtypes: datetime64[ns](1), float64(26), int64(21), object(2), string(1)\n",
      "memory usage: 1005.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Load data ---\n",
    "print(\"Loading data...\")\n",
    "train_full_df = pd.read_csv(TRAIN_FILE_PATH, parse_dates=[timestamp_column])\n",
    "test_full_df = pd.read_csv(TEST_FILE_PATH,  parse_dates=[timestamp_column])\n",
    "\n",
    "\n",
    "# Keep building_id as string (e.g., 'H001'); trim any whitespace\n",
    "for df in (train_full_df, test_full_df):\n",
    "    df[\"building_id\"] = df[\"building_id\"].astype(\"string\").str.strip()\n",
    "\n",
    "\n",
    "# --- 4) Save row_ids for submission (from TARGET rows in competition test) ---\n",
    "print(\"Saving row_ids for submission...\")\n",
    "if \"row_id\" in test_full_df.columns:\n",
    "    submission_ids = test_full_df.loc[test_full_df[\"role\"] == \"target\", [\"row_id\"]].copy()\n",
    "else:\n",
    "    submission_ids = None\n",
    "    print(\"Note: 'row_id' not found in test; skipping submission_ids extraction.\")\n",
    "\n",
    "# Keep everything for now (we need 'building_id' and 'role' for clean splits)\n",
    "train_df1 = train_full_df.copy()\n",
    "test_df1  = test_full_df.copy()\n",
    "\n",
    "# --- 5) Build chronological 80/10/10 window splits *per building* from TRAIN only ---\n",
    "# Use only INPUT rows to define each window's start time\n",
    "required = {\"building_id\", \"window_id\", \"role\", timestamp_column}\n",
    "assert required.issubset(train_df1.columns), \\\n",
    "    f\"Required columns missing for window split: {required - set(train_df1.columns)}\"\n",
    "\n",
    "winfo = (\n",
    "    train_df1.loc[train_df1[\"role\"] == \"input\", [\"building_id\", \"window_id\", timestamp_column]]\n",
    "    .groupby([\"building_id\", \"window_id\"], as_index=False)\n",
    "    .agg(window_start=(timestamp_column, \"min\"))\n",
    "    .sort_values([\"building_id\", \"window_start\"])\n",
    ")\n",
    "\n",
    "# Quick visibility\n",
    "n_buildings = winfo[\"building_id\"].nunique()\n",
    "n_windows   = len(winfo)\n",
    "print(f\"Found {n_windows} windows across {n_buildings} buildings in train.\")\n",
    "\n",
    "# ---- Integer 80/10/10 per building (chronological) ----\n",
    "train_window_ids, valid_window_ids, test_window_ids = set(), set(), set()\n",
    "\n",
    "for bld, g in winfo.groupby(\"building_id\", sort=False):\n",
    "    # g is already sorted by window_start from the earlier code\n",
    "    ids = g[\"window_id\"].tolist()\n",
    "    n = len(ids)\n",
    "\n",
    "    n_train = int(0.8 * n)   # integer count for train\n",
    "    n_valid = int(0.1 * n)   # integer count for valid\n",
    "    # remainder goes to test (ensures totals sum to n)\n",
    "    n_test  = n - n_train - n_valid\n",
    "\n",
    "    train_ids_b = ids[:n_train]\n",
    "    valid_ids_b = ids[n_train:n_train + n_valid]\n",
    "    test_ids_b  = ids[n_train + n_valid:]  # last chunk = remainder\n",
    "\n",
    "    train_window_ids.update(train_ids_b)\n",
    "    valid_window_ids.update(valid_ids_b)\n",
    "    test_window_ids.update(test_ids_b)\n",
    "\n",
    "print(f\"[Split per building] train={len(train_window_ids)}, \"\n",
    "      f\"valid={len(valid_window_ids)}, test={len(test_window_ids)}\")\n",
    "\n",
    "# -----------------------\n",
    "# Sanity checks (BEFORE subsampling)\n",
    "# -----------------------\n",
    "\n",
    "# 1) Window integrity: every window must have 168 'input' + 24 'target'\n",
    "_counts = train_df1.groupby([\"window_id\",\"role\"]).size().unstack(fill_value=0)\n",
    "assert \"input\" in _counts.columns and \"target\" in _counts.columns, \\\n",
    "    \"Expected roles 'input' and 'target' not found.\"\n",
    "assert _counts[\"input\"].eq(168).all(), \"Some windows do not have 168 input rows.\"\n",
    "assert _counts[\"target\"].eq(24).all(), \"Some windows do not have 24 target rows.\"\n",
    "\n",
    "# 2) Splits are disjoint (no overlap)\n",
    "assert len(train_window_ids & valid_window_ids) == 0, \"Train and Valid windows overlap.\"\n",
    "assert len(train_window_ids & test_window_ids) == 0,  \"Train and Test windows overlap.\"\n",
    "assert len(valid_window_ids & test_window_ids) == 0,  \"Valid and Test windows overlap.\"\n",
    "\n",
    "# 3) Coverage BEFORE subsample: all windows in train_df1 are assigned to exactly one split\n",
    "_all_window_ids = set(winfo[\"window_id\"])\n",
    "_union_before = train_window_ids | valid_window_ids | test_window_ids\n",
    "assert _union_before == _all_window_ids, \\\n",
    "    \"Split does not cover all windows before subsampling.\"\n",
    "\n",
    "# 4) Chronology per building: last train <= first valid <= first test (where applicable)\n",
    "_wstarts = (\n",
    "    train_df1.loc[train_df1[\"role\"] == \"input\", [\"building_id\",\"window_id\",\"timestamp\"]]\n",
    "    .groupby([\"building_id\",\"window_id\"], as_index=False)\n",
    "    .agg(window_start=(\"timestamp\",\"min\"))\n",
    ")\n",
    "\n",
    "def _per_bld_min(ids):\n",
    "    s = _wstarts[_wstarts.window_id.isin(ids)].groupby(\"building_id\").window_start\n",
    "    return s.min() if len(s) else pd.Series(dtype=\"datetime64[ns]\")\n",
    "\n",
    "def _per_bld_max(ids):\n",
    "    s = _wstarts[_wstarts.window_id.isin(ids)].groupby(\"building_id\").window_start\n",
    "    return s.max() if len(s) else pd.Series(dtype=\"datetime64[ns]\")\n",
    "\n",
    "_max_train = _per_bld_max(train_window_ids)\n",
    "_min_valid = _per_bld_min(valid_window_ids)\n",
    "_min_test  = _per_bld_min(test_window_ids)\n",
    "\n",
    "if not _max_train.empty and not _min_valid.empty:\n",
    "    _idx = _max_train.index.intersection(_min_valid.index)\n",
    "    assert (_max_train.loc[_idx] <= _min_valid.loc[_idx]).all(), \\\n",
    "        \"For some buildings, valid windows start before the last train window.\"\n",
    "if not _min_valid.empty and not _min_test.empty:\n",
    "    _idx = _min_valid.index.intersection(_min_test.index)\n",
    "    assert (_min_valid.loc[_idx] <= _min_test.loc[_idx]).all(), \\\n",
    "        \"For some buildings, test windows start before the first valid window.\"\n",
    "\n",
    "# Keep originals for post-subsample checks\n",
    "orig_train_window_ids = train_window_ids.copy()\n",
    "orig_valid_window_ids = valid_window_ids.copy()\n",
    "orig_test_window_ids  = test_window_ids.copy()\n",
    "\n",
    "# --- 6) (Optional) Subsample training windows to speed up experiments ---\n",
    "if 0 < subset_fraction < 1.0:\n",
    "    # Keep the earliest fraction within each building's *train* windows\n",
    "    keep_train = []\n",
    "    for bld, g in winfo[winfo[\"window_id\"].isin(orig_train_window_ids)].groupby(\"building_id\", sort=False):\n",
    "        ids = g[\"window_id\"].tolist()  # chronological\n",
    "        k = max(1, int(len(ids) * subset_fraction))\n",
    "        keep_train.extend(ids[:k])\n",
    "    train_window_ids = set(keep_train)\n",
    "    print(f\"[Subsample] subset_fraction={subset_fraction} -> train={len(train_window_ids)}\")\n",
    "\n",
    "    # -----------------------\n",
    "    # Sanity checks (AFTER subsampling)\n",
    "    # -----------------------\n",
    "\n",
    "    # 1) New train must be a subset of original train; still disjoint from valid/test\n",
    "    assert train_window_ids.issubset(orig_train_window_ids), \\\n",
    "        \"Subsampled train includes windows not in original train set.\"\n",
    "    assert len(train_window_ids & orig_valid_window_ids) == 0, \\\n",
    "        \"Train and Valid overlap after subsample.\"\n",
    "    assert len(train_window_ids & orig_test_window_ids) == 0, \\\n",
    "        \"Train and Test overlap after subsample.\"\n",
    "\n",
    "    # 2) Coverage is now a subset of all windows (because we dropped some train windows on purpose)\n",
    "    covered_after = train_window_ids | orig_valid_window_ids | orig_test_window_ids\n",
    "    dropped = _all_window_ids - covered_after\n",
    "    print(f\"[Info] Dropped {len(dropped)} original train windows due to subset_fraction.\")\n",
    "\n",
    "print(\"\\nCell 2 executed: Data loaded/merged and chronological splits created per building.\")\n",
    "train_df1.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9784d422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating time-based features...\n",
      "Encoding categorical features (region) with train-derived columns...\n",
      "Fitting preprocessor on INPUT rows of train+valid+test windows...\n",
      "Building datasets via ForecastDFDataset...\n",
      "Datasets: train = 10720 | valid = 1310 | test = 3525\n",
      "Context/Pred: 168 24\n",
      "\n",
      "Cell 3 done: global scaling OFF; model will do per-window instance norm.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Features → Splits → Preprocessor → Datasets ---\n",
    "print(\"Creating time-based features...\")\n",
    "\n",
    "def create_time_features(df, timestamp_col):\n",
    "    df = df.copy()\n",
    "    df[\"hour\"]       = df[timestamp_col].dt.hour\n",
    "    df[\"dayofweek\"]  = df[timestamp_col].dt.dayofweek\n",
    "    df[\"month\"]      = df[timestamp_col].dt.month\n",
    "    df[\"hour_sin\"]   = np.sin(2*np.pi*df[\"hour\"]/24)\n",
    "    df[\"hour_cos\"]   = np.cos(2*np.pi*df[\"hour\"]/24)\n",
    "    return df\n",
    "\n",
    "train_df1 = create_time_features(train_df1, timestamp_column)\n",
    "test_df1  = create_time_features(test_df1,  timestamp_column)\n",
    "\n",
    "# --- 3.1 Split rows by the window sets from Cell 2 ---\n",
    "train_rows = train_df1[\"window_id\"].isin(train_window_ids)\n",
    "valid_rows = train_df1[\"window_id\"].isin(valid_window_ids)\n",
    "test_rows_comp = test_df1[\"window_id\"].notna() if \"window_id\" in test_df1.columns else pd.Series(True, index=test_df1.index)\n",
    "\n",
    "train_split = train_df1.loc[train_rows].copy()\n",
    "valid_split = train_df1.loc[valid_rows].copy()\n",
    "test_split  = test_df1.loc[test_rows_comp].copy()   # competition/test file\n",
    "\n",
    "# --- 3.2 One-hot 'region' using TRAIN categories only (prevents leakage) ---\n",
    "print(\"Encoding categorical features (region) with train-derived columns...\")\n",
    "train_region_dum = pd.get_dummies(train_split[\"region\"], prefix=\"region\")\n",
    "region_cols = sorted(train_region_dum.columns)\n",
    "\n",
    "def _apply_region_dummies(df, region_cols):\n",
    "    Xr = pd.get_dummies(df[\"region\"], prefix=\"region\")\n",
    "    return Xr.reindex(columns=region_cols, fill_value=0)\n",
    "\n",
    "train_region = _apply_region_dummies(train_split, region_cols)\n",
    "valid_region = _apply_region_dummies(valid_split, region_cols)\n",
    "test_region  = _apply_region_dummies(test_split,  region_cols)\n",
    "\n",
    "# --- 3.3 Assemble frames with numeric features + region dummies (exclude identifiers) ---\n",
    "exclude = {\"building_id\",\"window_id\",\"row_id\",\"role\",\"timestamp\",\"meter_reading\",\"region\"}\n",
    "\n",
    "feat_cols_no_region = [c for c in train_split.columns if c not in exclude]\n",
    "feat_cols_no_region = sorted(feat_cols_no_region)   # keep consistent order\n",
    "\n",
    "# Train\n",
    "train_ready = pd.concat(\n",
    "    [train_split[[\"window_id\",\"timestamp\",\"meter_reading\",\"role\"] + feat_cols_no_region], train_region],\n",
    "    axis=1\n",
    ")\n",
    "# Valid\n",
    "valid_ready = pd.concat(\n",
    "    [valid_split[[\"window_id\",\"timestamp\",\"meter_reading\",\"role\"] + feat_cols_no_region], valid_region],\n",
    "    axis=1\n",
    ")\n",
    "# Test (competition)\n",
    "base_cols_test = [\"window_id\",\"timestamp\",\"meter_reading\"] + feat_cols_no_region\n",
    "base_cols_test = [c for c in base_cols_test if c in test_split.columns]\n",
    "test_ready  = pd.concat([test_split[base_cols_test], test_region], axis=1)\n",
    "\n",
    "# --- 3.4 Optional numeric imputation using TRAIN+INPUT medians (no leakage) ---\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "control_columns = feat_cols_no_region + region_cols\n",
    "numeric_controls = [c for c in control_columns if c in train_ready.columns and is_numeric_dtype(train_ready[c])]\n",
    "\n",
    "fit_mask = (train_ready[\"role\"] == \"input\") if \"role\" in train_ready.columns else pd.Series(False, index=train_ready.index)\n",
    "medians = train_ready.loc[fit_mask, numeric_controls].median(numeric_only=True)\n",
    "\n",
    "def _fill_missing(df, cols, med):\n",
    "    df = df.copy()\n",
    "    if len(med):\n",
    "        cols_present = [c for c in cols if c in df.columns]\n",
    "        df[cols_present] = df[cols_present].fillna(med.reindex(cols_present))\n",
    "    return df\n",
    "\n",
    "train_ready = _fill_missing(train_ready, numeric_controls, medians)\n",
    "valid_ready = _fill_missing(valid_ready, numeric_controls, medians)\n",
    "test_ready  = _fill_missing(test_ready,  numeric_controls, medians)\n",
    "\n",
    "# --- 3.4b Enforce numeric dtypes (prevents np.object_ later) ---\n",
    "def _to_float32(df, cols):\n",
    "    cols_present = [c for c in cols if c in df.columns]\n",
    "    if cols_present:\n",
    "        df[cols_present] = df[cols_present].apply(pd.to_numeric, errors=\"coerce\").astype(np.float32)\n",
    "    return df\n",
    "\n",
    "for _df in (train_ready, valid_ready, test_ready):\n",
    "    _df[\"meter_reading\"] = pd.to_numeric(_df[\"meter_reading\"], errors=\"coerce\").astype(np.float32)\n",
    "\n",
    "train_ready = _to_float32(train_ready, control_columns)\n",
    "valid_ready = _to_float32(valid_ready, control_columns)\n",
    "test_ready  = _to_float32(test_ready,  control_columns)\n",
    "\n",
    "# --- 3.5 Fit preprocessor on INPUT rows only (no scaling here) ---\n",
    "print(\"Fitting preprocessor on INPUT rows of train+valid+test windows...\")\n",
    "\n",
    "base_cols_test_with_role = [\"window_id\",\"timestamp\",\"meter_reading\",\"role\"] + feat_cols_no_region\n",
    "base_cols_test_with_role = [c for c in base_cols_test_with_role if c in test_split.columns]\n",
    "test_ready_with_role = pd.concat([test_split[base_cols_test_with_role], test_region], axis=1)\n",
    "test_ready_with_role = _fill_missing(test_ready_with_role, numeric_controls, medians)\n",
    "test_ready_with_role[\"meter_reading\"] = pd.to_numeric(test_ready_with_role[\"meter_reading\"], errors=\"coerce\").astype(np.float32)\n",
    "test_ready_with_role = _to_float32(test_ready_with_role, control_columns)\n",
    "\n",
    "tsp = TimeSeriesPreprocessor(\n",
    "    id_columns=[\"window_id\"],\n",
    "    timestamp_column=\"timestamp\",\n",
    "    target_columns=[\"meter_reading\"],\n",
    "    control_columns=control_columns,\n",
    "    context_length=context_length,\n",
    "    prediction_length=prediction_length,\n",
    "    scaling=False,                 # <<< CHANGED: disable global scaler\n",
    "    freq=\"H\",\n",
    ")\n",
    "\n",
    "fit_df = pd.concat(\n",
    "    [\n",
    "        train_ready.loc[train_ready[\"role\"] == \"input\"],\n",
    "        valid_ready.loc[valid_ready[\"role\"] == \"input\"],\n",
    "        test_ready_with_role.loc[test_ready_with_role[\"role\"] == \"input\"],\n",
    "    ],\n",
    "    ignore_index=True\n",
    ").drop(columns=[\"role\"])\n",
    "\n",
    "tsp.train(fit_df)\n",
    "\n",
    "# --- 3.6 Drop 'role' and build datasets ---\n",
    "train_ready_norole = train_ready.drop(columns=[\"role\"])\n",
    "valid_ready_norole = valid_ready.drop(columns=[\"role\"])\n",
    "\n",
    "print(\"Building datasets via ForecastDFDataset...\")\n",
    "from tsfm_public import ForecastDFDataset\n",
    "\n",
    "dataset_params = dict(\n",
    "    timestamp_column=\"timestamp\",\n",
    "    id_columns=[\"window_id\"],\n",
    "    target_columns=[\"meter_reading\"],\n",
    "    control_columns=control_columns,\n",
    "    context_length=context_length,\n",
    "    prediction_length=prediction_length,\n",
    ")\n",
    "\n",
    "train_proc = tsp.preprocess(train_ready_norole)\n",
    "valid_proc = tsp.preprocess(valid_ready_norole)\n",
    "test_proc  = tsp.preprocess(test_ready)\n",
    "\n",
    "# sanity: ensure no object dtypes slipped through\n",
    "def _assert_no_object(df, name):\n",
    "    bad = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    if bad:\n",
    "        raise TypeError(f\"{name} has object dtypes: {bad[:10]}\")\n",
    "\n",
    "_assert_no_object(train_proc, \"train_proc\")\n",
    "_assert_no_object(valid_proc, \"valid_proc\")\n",
    "_assert_no_object(test_proc,  \"test_proc\")\n",
    "\n",
    "train_dataset = ForecastDFDataset(train_proc, **dataset_params)\n",
    "valid_dataset = ForecastDFDataset(valid_proc, **dataset_params)\n",
    "test_dataset  = ForecastDFDataset(test_proc,  **dataset_params)\n",
    "\n",
    "print(\"Datasets:\",\n",
    "      \"train =\", len(train_dataset),\n",
    "      \"| valid =\", len(valid_dataset),\n",
    "      \"| test =\", len(test_dataset))\n",
    "print(\"Context/Pred:\", tsp.context_length, tsp.prediction_length)\n",
    "\n",
    "assert len(train_dataset) == len(train_window_ids)\n",
    "assert len(valid_dataset) == len(valid_window_ids)\n",
    "\n",
    "print(\"\\nCell 3 done: global scaling OFF; model will do per-window instance norm.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0256ade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TTM (keep checkpoint CL/patch; adapt head to FL=H)…\n",
      "[data] L=168, H=24, C=53\n",
      "[ckpt] sl=512, fl=96, patch_len=64\n",
      "[FLA] pruned head out_features: 96 -> 24\n",
      "[fix] decoder gating attn layers reset: 0\n",
      "Backbone scaler: TinyTimeMixerStdScaler\n",
      "Cell 4 OK.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 4 (additions marked) ---\n",
    "from tsfm_public import TinyTimeMixerForPrediction, TinyTimeMixerConfig\n",
    "import torch, torch.nn as nn\n",
    "\n",
    "print(\"Loading TTM (keep checkpoint CL/patch; adapt head to FL=H)…\")\n",
    "\n",
    "CKPT = \"ibm-granite/granite-timeseries-ttm-r2\"\n",
    "\n",
    "s0 = train_dataset[0]\n",
    "L  = int(s0[\"past_values\"].shape[0])\n",
    "H  = int(s0[\"future_values\"].shape[0])\n",
    "C  = int(s0[\"past_values\"].shape[1])\n",
    "print(f\"[data] L={L}, H={H}, C={C}\")\n",
    "\n",
    "cfg = TinyTimeMixerConfig.from_pretrained(CKPT)\n",
    "cfg.head_dropout = 0.2  # add some head dropout\n",
    "# (optional, if available) keep model-side instance norm ON\n",
    "if hasattr(cfg, \"scaling\") and (cfg.scaling is None or cfg.scaling == \"none\"):\n",
    "    cfg.scaling = \"std\"   # ensure built-in per-window std-scaler\n",
    "\n",
    "ckpt_sl        = int(getattr(cfg, \"context_length\", 512))\n",
    "ckpt_fl        = int(getattr(cfg, \"prediction_length\", 96))\n",
    "ckpt_patch_len = int(getattr(cfg, \"patch_length\", 64))\n",
    "print(f\"[ckpt] sl={ckpt_sl}, fl={ckpt_fl}, patch_len={ckpt_patch_len}\")\n",
    "\n",
    "for k in (\"num_input_channels\", \"nvars\", \"num_channels\", \"decoder_num_channels\", \"decoder_nvars\"):\n",
    "    if hasattr(cfg, k):\n",
    "        setattr(cfg, k, C)\n",
    "\n",
    "forecast_model = TinyTimeMixerForPrediction.from_pretrained(\n",
    "    CKPT,\n",
    "    config=cfg,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# prune head to H (unchanged) …\n",
    "def _prune_linear_out_rows(linear: nn.Linear, keep_rows: int):\n",
    "    new = nn.Linear(linear.in_features, keep_rows, bias=(linear.bias is not None))\n",
    "    with torch.no_grad():\n",
    "        new.weight.copy_(linear.weight[:keep_rows, :])\n",
    "        if linear.bias is not None:\n",
    "            new.bias.copy_(linear.bias[:keep_rows])\n",
    "    return new\n",
    "\n",
    "if hasattr(forecast_model.head, \"base_forecast_block\") and isinstance(forecast_model.head.base_forecast_block, nn.Linear):\n",
    "    lin = forecast_model.head.base_forecast_block\n",
    "    if lin.out_features != H and lin.out_features >= H:\n",
    "        forecast_model.head.base_forecast_block = _prune_linear_out_rows(lin, H)\n",
    "        print(f\"[FLA] pruned head out_features: {lin.out_features} -> {H}\")\n",
    "\n",
    "# gating safety (unchanged) …\n",
    "fixed = 0\n",
    "for name, mod in forecast_model.named_modules():\n",
    "    if name.endswith(\"channel_feature_mixer.gating_block.attn_layer\") and isinstance(mod, nn.Linear):\n",
    "        if mod.in_features != C or mod.out_features != C:\n",
    "            parent = forecast_model\n",
    "            for part in name.rsplit(\".\", 1)[0].split(\".\"):\n",
    "                parent = getattr(parent, part)\n",
    "            setattr(parent, \"attn_layer\", nn.Linear(C, C, bias=True))\n",
    "            fixed += 1\n",
    "print(f\"[fix] decoder gating attn layers reset: {fixed}\")\n",
    "\n",
    "forecast_model.config.prediction_length = H\n",
    "\n",
    "# freeze encoder; train decoder+head\n",
    "for p in forecast_model.parameters(): p.requires_grad = False\n",
    "for n, p in forecast_model.named_parameters():\n",
    "    if n.startswith(\"decoder.\") or n.startswith(\"head.\"):\n",
    "        p.requires_grad = True\n",
    "\n",
    "# >>> Sanity: print active scaler in the backbone\n",
    "try:\n",
    "    print(\"Backbone scaler:\", type(forecast_model.backbone.scaler).__name__)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# smoke test (unchanged) …\n",
    "with torch.no_grad():\n",
    "    pv  = s0[\"past_values\"].float()\n",
    "    pom = s0[\"past_observed_mask\"].float()\n",
    "    pad = ckpt_sl - pv.shape[0]\n",
    "    assert pad >= 0\n",
    "    filler = pv.mean(dim=0)\n",
    "    pv  = torch.cat([filler.expand(pad, C), pv], dim=0).unsqueeze(0)\n",
    "    pom = torch.cat([torch.zeros(pad, C),  pom], dim=0).unsqueeze(0)\n",
    "    fv  = s0[\"future_values\"].unsqueeze(0).float()\n",
    "    fom = s0[\"future_observed_mask\"].unsqueeze(0).float()\n",
    "    _   = forecast_model(past_values=pv, future_values=fv,\n",
    "                         past_observed_mask=pom, future_observed_mask=fom)\n",
    "\n",
    "print(\"Cell 4 OK.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0258d9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using loss = mse\n",
      "Phase 1: head-only fine-tuning…\n",
      "[make_trainer] head params: 24,600 | decoder params: 0 | LRs: head=0.001, dec=0.0 | epochs=8, batch=32x2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1344' max='1344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1344/1344 00:28, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.256200</td>\n",
       "      <td>0.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.104300</td>\n",
       "      <td>0.085419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.090800</td>\n",
       "      <td>0.079416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.076066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.074934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.081200</td>\n",
       "      <td>0.074268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.080800</td>\n",
       "      <td>0.074029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.074072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase 1] best_checkpoint=./ttm_ft_phase1_head/checkpoint-1176 | best_eval_loss=0.07402917742729187\n",
      "Phase 2: head+decoder fine-tuning…\n",
      "[make_trainer] head params: 24,600 | decoder params: 191,296 | LRs: head=0.0008, dec=0.0002 | epochs=28, batch=32x2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3696' max='4704' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3696/4704 01:40 < 00:27, 36.72 it/s, Epoch 22/28]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.073700</td>\n",
       "      <td>0.069196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.063400</td>\n",
       "      <td>0.066404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.059100</td>\n",
       "      <td>0.065382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.056500</td>\n",
       "      <td>0.063840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.063151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.054000</td>\n",
       "      <td>0.062671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.053400</td>\n",
       "      <td>0.062039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.061870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.061852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.062039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.061663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.061366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.061192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.061576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.061833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.061762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.061060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.061229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.051000</td>\n",
       "      <td>0.061466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.061288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.061184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.050700</td>\n",
       "      <td>0.061189</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Phase 2] best_checkpoint=./ttm_ft_phase2_dec/checkpoint-2856 | best_eval_loss=0.06106005236506462\n",
      "✅ Training complete (ckpt CL preserved via padding, FL=24, RPT on, EarlyStopping + best model by eval_loss).\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 5 (ES + Best Model on eval_loss): pad-to-ckpt-CL collator + RPT; 2-phase FT (head → head+decoder)\n",
    "import math, torch, inspect\n",
    "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback, get_cosine_schedule_with_warmup\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "forecast_model.to(device)\n",
    "set_seed(SEED)\n",
    "\n",
    "# 1) Force MSE inside TTM (so Trainer's eval_loss == MSE)\n",
    "if hasattr(forecast_model, \"loss\"):\n",
    "    forecast_model.loss = \"mse\"\n",
    "if hasattr(forecast_model.config, \"loss\"):\n",
    "    forecast_model.config.loss = \"mse\"\n",
    "print(\"Using loss =\", getattr(forecast_model, \"loss\", None))\n",
    "\n",
    "CKPT_SL = int(getattr(forecast_model.config, \"context_length\", 512))\n",
    "FREQ_ID = 1  # set to your dataset’s resolution id (e.g., hourly=1)\n",
    "\n",
    "# 2) Collator: left-pad 168→CKPT_SL, mask pad, add freq_token; only forward-allowed keys\n",
    "_forward_allowed = set(inspect.signature(forecast_model.forward).parameters.keys())\n",
    "def pad_to_ckpt_collate(batch):\n",
    "    out = {}\n",
    "    C = batch[0][\"past_values\"].shape[-1]\n",
    "    pv_list, pom_list = [], []\n",
    "    for s in batch:\n",
    "        pv  = s[\"past_values\"].float()\n",
    "        pom = s[\"past_observed_mask\"].float()\n",
    "        pad = CKPT_SL - pv.shape[0]\n",
    "        if pad < 0:\n",
    "            pv, pom = pv[-CKPT_SL:], pom[-CKPT_SL:]\n",
    "            pad = 0\n",
    "        fill   = pv.mean(dim=0)\n",
    "        pv_pad = torch.cat([fill.expand(pad, C), pv], dim=0)\n",
    "        om_pad = torch.cat([torch.zeros(pad, C),  pom], dim=0)\n",
    "        pv_list.append(pv_pad)\n",
    "        pom_list.append(om_pad)\n",
    "\n",
    "    out[\"past_values\"]        = torch.stack(pv_list, 0)\n",
    "    out[\"past_observed_mask\"] = torch.stack(pom_list, 0)\n",
    "    if \"future_values\" in batch[0]:\n",
    "        out[\"future_values\"] = torch.stack([s[\"future_values\"].float() for s in batch], 0)\n",
    "    if \"future_observed_mask\" in batch[0]:\n",
    "        out[\"future_observed_mask\"] = torch.stack([s[\"future_observed_mask\"].float() for s in batch], 0)\n",
    "\n",
    "    # Resolution Prefix Token (RPT)\n",
    "    fts = []\n",
    "    for s in batch:\n",
    "        ft = s.get(\"freq_token\", None)\n",
    "        if ft is None:\n",
    "            ft = torch.tensor(FREQ_ID, dtype=torch.long)\n",
    "        elif not torch.is_tensor(ft):\n",
    "            ft = torch.tensor(int(ft), dtype=torch.long)\n",
    "        fts.append(ft)\n",
    "    out[\"freq_token\"] = torch.stack(fts, 0)\n",
    "\n",
    "    # Make model compute & return loss during train/eval\n",
    "    if \"return_loss\" in _forward_allowed:\n",
    "        out[\"return_loss\"] = True\n",
    "    if \"return_dict\" in _forward_allowed:\n",
    "        out[\"return_dict\"] = True\n",
    "\n",
    "    return {k: v for k, v in out.items() if k in _forward_allowed}\n",
    "\n",
    "# 3) Build Trainer per phase with AdamW + cosine schedule + early stopping on eval_loss\n",
    "def make_trainer(model, train_ds, eval_ds, lr_head, lr_dec, epochs, output_dir, bs=256, ga=2, wd=1e-2):\n",
    "    head_named = [(n,p) for n,p in model.named_parameters() if p.requires_grad and n.startswith(\"head.\")]\n",
    "    dec_named  = [(n,p) for n,p in model.named_parameters() if p.requires_grad and n.startswith(\"decoder.\")]\n",
    "\n",
    "    def _split_decay(named_params):\n",
    "        decay, no_decay = [], []\n",
    "        for n, p in named_params:\n",
    "            if n.endswith(\".bias\") or (\"norm\" in n.lower()) or (\"layernorm\" in n.lower()):\n",
    "                no_decay.append(p)\n",
    "            else:\n",
    "                decay.append(p)\n",
    "        return decay, no_decay\n",
    "\n",
    "    head_decay, head_no_decay = _split_decay(head_named)\n",
    "    dec_decay,  dec_no_decay  = _split_decay(dec_named)\n",
    "\n",
    "    groups = []\n",
    "    if head_decay:     groups.append({\"params\": head_decay,     \"lr\": lr_head, \"weight_decay\": wd})\n",
    "    if head_no_decay:  groups.append({\"params\": head_no_decay,  \"lr\": lr_head, \"weight_decay\": 0.0})\n",
    "    if dec_decay:      groups.append({\"params\": dec_decay,      \"lr\": lr_dec,  \"weight_decay\": wd})\n",
    "    if dec_no_decay:   groups.append({\"params\": dec_no_decay,   \"lr\": lr_dec,  \"weight_decay\": 0.0})\n",
    "\n",
    "    use_fused = hasattr(torch.optim, \"AdamW\") and \"fused\" in torch.optim.AdamW.__init__.__code__.co_varnames\n",
    "    optimizer = torch.optim.AdamW(groups, fused=use_fused)\n",
    "\n",
    "    steps_per_epoch = max(1, math.ceil(len(train_ds) / (bs * ga)))\n",
    "    t_total = steps_per_epoch * epochs\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=int(0.1 * t_total), num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    sm_major = torch.cuda.get_device_capability()[0] if torch.cuda.is_available() else 0\n",
    "    use_bf16 = torch.cuda.is_available() and sm_major >= 8\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        learning_rate=max(lr_head, lr_dec, 1e-8),  # real LRs set per param group\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs,\n",
    "        gradient_accumulation_steps=ga,\n",
    "\n",
    "        # eval/log/save each epoch\n",
    "        eval_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "\n",
    "        # >>> keep the best checkpoint by eval_loss (MSE) <<<\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "\n",
    "        # keep our keys; collator already filters to forward-allowed\n",
    "        remove_unused_columns=False,\n",
    "\n",
    "        dataloader_num_workers=8,\n",
    "        bf16=use_bf16,\n",
    "        fp16=(torch.cuda.is_available() and not use_bf16),\n",
    "        max_grad_norm=1.0,\n",
    "        report_to=[],\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    es = EarlyStoppingCallback(early_stopping_patience=5, early_stopping_threshold=0.0)\n",
    "\n",
    "    print(\n",
    "        f\"[make_trainer] head params: {sum(p.numel() for _,p in head_named):,} | \"\n",
    "        f\"decoder params: {sum(p.numel() for _,p in dec_named):,} | \"\n",
    "        f\"LRs: head={lr_head}, dec={lr_dec} | epochs={epochs}, batch={bs}x{ga}\"\n",
    "    )\n",
    "\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=eval_ds,\n",
    "        data_collator=pad_to_ckpt_collate,\n",
    "        optimizers=(optimizer, scheduler),\n",
    "        callbacks=[es],\n",
    "    )\n",
    "\n",
    "# 4) Phase settings\n",
    "bs, ga = 32, 2\n",
    "epochs_phase1 = 8\n",
    "epochs_phase2 = 28\n",
    "\n",
    "# ----------------- Phase 1: HEAD ONLY -----------------\n",
    "for n, p in forecast_model.named_parameters():\n",
    "    if n.startswith(\"decoder.\"): p.requires_grad = False\n",
    "    if n.startswith(\"head.\"):    p.requires_grad = True\n",
    "\n",
    "print(\"Phase 1: head-only fine-tuning…\")\n",
    "trainer = make_trainer(\n",
    "    forecast_model, train_dataset, valid_dataset,\n",
    "    lr_head=1e-3, lr_dec=0.0, epochs=epochs_phase1,\n",
    "    output_dir=\"./ttm_ft_phase1_head\", bs=bs, ga=ga\n",
    ")\n",
    "trainer.train()\n",
    "print(f\"[Phase 1] best_checkpoint={trainer.state.best_model_checkpoint} | best_eval_loss={trainer.state.best_metric}\")\n",
    "\n",
    "# ----------------- Phase 2: HEAD + DECODER -----------------\n",
    "for n, p in forecast_model.named_parameters():\n",
    "    if n.startswith(\"decoder.\"): p.requires_grad = True\n",
    "\n",
    "print(\"Phase 2: head+decoder fine-tuning…\")\n",
    "trainer = make_trainer(\n",
    "    forecast_model, train_dataset, valid_dataset,\n",
    "    lr_head=8e-4, lr_dec=2e-4, epochs=epochs_phase2,\n",
    "    output_dir=\"./ttm_ft_phase2_dec\", bs=bs, ga=ga\n",
    ")\n",
    "trainer.train()\n",
    "print(f\"[Phase 2] best_checkpoint={trainer.state.best_model_checkpoint} | best_eval_loss={trainer.state.best_metric}\")\n",
    "\n",
    "print(\"✅ Training complete (ckpt CL preserved via padding, FL=24, RPT on, EarlyStopping + best model by eval_loss).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf02b890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best ckpt: ./ttm_ft_phase2_dec/checkpoint-2856\n"
     ]
    }
   ],
   "source": [
    "print(\"Best ckpt:\", trainer.state.best_model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23c02135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ wrote val_preds_seed79_r2.csv and val_ground_truth.csv | rows=31440\n"
     ]
    }
   ],
   "source": [
    "# --- Cell A: write validation preds & GT with per-window de-normalization (paper-style) ---\n",
    "import torch, numpy as np, pandas as pd, os\n",
    "\n",
    "forecast_model.eval()\n",
    "device = next(forecast_model.parameters()).device\n",
    "\n",
    "SL = int(getattr(forecast_model.config, \"context_length\", 512))\n",
    "H  = int(getattr(forecast_model.config, \"prediction_length\", 24))\n",
    "CONTEXT_LEN = int(globals().get(\"context_length\", 168))\n",
    "tgt_idx = getattr(tsp, \"prediction_channel_indices\", [0])[0]  # target channel\n",
    "\n",
    "# 0) ordered window_ids for valid set (1 sample == 1 window)\n",
    "val_window_ids = valid_proc[\"window_id\"].astype(int).drop_duplicates().tolist()\n",
    "assert len(val_window_ids) == len(valid_dataset), \"valid ids ↔ dataset length mismatch\"\n",
    "\n",
    "def _pad_or_crop_to_SL(pv, pom, SL):\n",
    "    T, C = pv.shape\n",
    "    if T == SL: return pv, pom\n",
    "    if T > SL:  return pv[-SL:, :], pom[-SL:, :]\n",
    "    pad = SL - T\n",
    "    filler = pv.mean(dim=0) if T > 0 else torch.zeros(C, dtype=pv.dtype, device=pv.device)\n",
    "    pv2  = torch.cat([filler.expand(pad, C), pv], dim=0)\n",
    "    pom2 = torch.cat([torch.zeros(pad, C, dtype=pom.dtype, device=pom.device), pom], dim=0)\n",
    "    return pv2, pom2\n",
    "\n",
    "def _extract_forecast(outputs):\n",
    "    # robust across TTM variants\n",
    "    if isinstance(outputs, dict):\n",
    "        for k in (\"forecast\",\"forecasts\",\"prediction_outputs\",\"predictions\",\"mean\",\"loc\"):\n",
    "            if k in outputs: \n",
    "                v = outputs[k]\n",
    "                return v[0] if isinstance(v,(list,tuple)) and torch.is_tensor(v[0]) else v\n",
    "    for k in (\"forecast\",\"forecasts\",\"prediction_outputs\",\"predictions\",\"mean\",\"loc\"):\n",
    "        if hasattr(outputs, k):\n",
    "            v = getattr(outputs, k)\n",
    "            return v[0] if isinstance(v,(list,tuple)) and torch.is_tensor(v[0]) else v\n",
    "    if isinstance(outputs,(list,tuple)) and torch.is_tensor(outputs[0]): \n",
    "        return outputs[0]\n",
    "    raise RuntimeError(\"forecast tensor not found in model output\")\n",
    "\n",
    "ids, yhat_all, ytrue_all = [], [], []\n",
    "bs, N = 64, len(valid_dataset)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in range(0, N, bs):\n",
    "        end   = min(N, start + bs)\n",
    "        batch = [valid_dataset[i] for i in range(start, end)]\n",
    "        wids  = val_window_ids[start:end]\n",
    "\n",
    "        pv_list, pom_list, fv_list, mean_list, std_list = [], [], [], [], []\n",
    "        for s in batch:\n",
    "            pv  = torch.as_tensor(s[\"past_values\"],        dtype=torch.float32, device=device)  # [Lp,C]\n",
    "            pom = torch.as_tensor(s[\"past_observed_mask\"], dtype=torch.float32, device=device)  # [Lp,C]\n",
    "            fv  = torch.as_tensor(s[\"future_values\"],      dtype=torch.float32, device=device)  # [H ,C]\n",
    "            fom = torch.as_tensor(s[\"future_observed_mask\"], dtype=torch.float32, device=device)  # [H ,C]\n",
    "\n",
    "            # instance stats from first CONTEXT_LEN past steps\n",
    "            Lp, C = pv.shape\n",
    "            ctx   = min(CONTEXT_LEN, Lp)\n",
    "            pv_c, pom_c = pv[:ctx, :], pom[:ctx, :]\n",
    "\n",
    "            denom = torch.clamp(pom_c.sum(dim=0, keepdim=True), min=1.0)       # (1,C)\n",
    "            mean  = (pv_c * pom_c).sum(dim=0, keepdim=True) / denom            # (1,C)\n",
    "            var   = ((pv_c - mean)**2 * pom_c).sum(dim=0, keepdim=True) / denom\n",
    "            std   = torch.sqrt(torch.clamp(var, min=1e-6))                     # (1,C)\n",
    "\n",
    "            # normalize past & future (paper)\n",
    "            pv_n = (pv - mean) / std\n",
    "            fv_n = (fv - mean) / std\n",
    "\n",
    "            # left-pad to ckpt CL\n",
    "            pv_n, pom = _pad_or_crop_to_SL(pv_n, pom, SL)\n",
    "\n",
    "            pv_list.append(pv_n)\n",
    "            pom_list.append(pom)\n",
    "            fv_list.append(fv_n)\n",
    "            mean_list.append(mean)\n",
    "            std_list.append(std)\n",
    "\n",
    "        model_inputs = {\n",
    "            \"past_values\":        torch.stack(pv_list,  0),   # [B,SL,C]\n",
    "            \"past_observed_mask\": torch.stack(pom_list, 0),   # [B,SL,C]\n",
    "            \"return_dict\": True, \"return_loss\": False\n",
    "        }\n",
    "        outputs = forecast_model(**model_inputs)\n",
    "        yhat = _extract_forecast(outputs)                     # [B,H] or [B,H,1]/[B,H,C]\n",
    "\n",
    "        # ensure [B,H] on target channel\n",
    "        if yhat.dim() == 3 and yhat.size(-1) == 1:\n",
    "            yhat = yhat.squeeze(-1)\n",
    "        elif yhat.dim() == 3 and yhat.size(-1) > 1:\n",
    "            yhat = yhat[..., tgt_idx]\n",
    "        assert yhat.dim() == 2 and yhat.size(1) == H\n",
    "\n",
    "        yhat = yhat.detach().cpu()\n",
    "        fv_n = torch.stack(fv_list, 0).detach().cpu()        # [B,H,C]\n",
    "        mean = torch.stack(mean_list, 0).detach().cpu()      # [B,1,C]\n",
    "        std  = torch.stack(std_list, 0).detach().cpu()       # [B,1,C]\n",
    "\n",
    "        # grab target channel for GT\n",
    "        ytrue_n = fv_n[..., tgt_idx]                         # [B,H]\n",
    "\n",
    "        # de-normalize both pred & true: x = x_norm * std + mean  (channel-wise)\n",
    "        mean_t = mean[..., tgt_idx]                          # [B,1]\n",
    "        std_t  = std[..., tgt_idx]                           # [B,1]\n",
    "        yhat_u  = (yhat * std_t)  + mean_t                   # [B,H]\n",
    "        ytrue_u = (ytrue_n * std_t) + mean_t                 # [B,H]\n",
    "\n",
    "        # flatten & attach ids\n",
    "        for wid, seq_p, seq_t in zip(wids, yhat_u.numpy(), ytrue_u.numpy()):\n",
    "            ids.extend([f\"{int(wid)}_{h}\" for h in range(1, H+1)])\n",
    "            yhat_all.extend(seq_p.tolist())\n",
    "            ytrue_all.extend(seq_t.tolist())\n",
    "\n",
    "# write files\n",
    "pred_path = f\"val_preds_seed{SEED}_r2.csv\"\n",
    "gt_path   = \"val_ground_truth.csv\"\n",
    "\n",
    "pd.DataFrame({\"id\": ids, \"y_hat\":  np.asarray(yhat_all,  dtype=np.float32)}).to_csv(pred_path, index=False)\n",
    "\n",
    "# write GT (same across seeds; overwrite safely)\n",
    "gt_df = pd.DataFrame({\"id\": ids, \"y_true\": np.asarray(ytrue_all, dtype=np.float32)})\n",
    "if (not os.path.exists(gt_path)) or (pd.read_csv(gt_path).shape != gt_df.shape):\n",
    "    gt_df.to_csv(gt_path, index=False)\n",
    "print(f\"✅ wrote {pred_path} and {gt_path} | rows={len(ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f7a4e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL: 731480 | TRAINABLE: 215896\n"
     ]
    }
   ],
   "source": [
    "total_params     = sum(p.numel() for p in forecast_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in forecast_model.parameters() if p.requires_grad)\n",
    "print(\"TOTAL:\", total_params, \"| TRAINABLE:\", trainable_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09da49c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[model] context_length=512, prediction_length=24\n",
      "Submission shape: (84600, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>meter_reading</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>169</td>\n",
       "      <td>0.347524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>170</td>\n",
       "      <td>0.364863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>171</td>\n",
       "      <td>0.374867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>172</td>\n",
       "      <td>0.371532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>173</td>\n",
       "      <td>0.355526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>174</td>\n",
       "      <td>0.390872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>175</td>\n",
       "      <td>0.541342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>176</td>\n",
       "      <td>0.753001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>177</td>\n",
       "      <td>0.785012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>178</td>\n",
       "      <td>0.568936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>179</td>\n",
       "      <td>0.468233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>180</td>\n",
       "      <td>0.426885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>181</td>\n",
       "      <td>0.407545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>182</td>\n",
       "      <td>0.408212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>183</td>\n",
       "      <td>0.415548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>184</td>\n",
       "      <td>0.417548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>185</td>\n",
       "      <td>0.442891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>186</td>\n",
       "      <td>0.509581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>187</td>\n",
       "      <td>0.625622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>188</td>\n",
       "      <td>0.801018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>189</td>\n",
       "      <td>0.926396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>190</td>\n",
       "      <td>0.931731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>191</td>\n",
       "      <td>0.819691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>192</td>\n",
       "      <td>0.659634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>361</td>\n",
       "      <td>0.793468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>362</td>\n",
       "      <td>0.699429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>363</td>\n",
       "      <td>0.668540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>364</td>\n",
       "      <td>0.623462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>365</td>\n",
       "      <td>0.623917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>366</td>\n",
       "      <td>0.669226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>367</td>\n",
       "      <td>0.787977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>368</td>\n",
       "      <td>0.860050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>369</td>\n",
       "      <td>0.727572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>370</td>\n",
       "      <td>0.598697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>371</td>\n",
       "      <td>0.500368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>372</td>\n",
       "      <td>0.433099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>373</td>\n",
       "      <td>0.411133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>374</td>\n",
       "      <td>0.396032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>375</td>\n",
       "      <td>0.380931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>376</td>\n",
       "      <td>0.402896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>377</td>\n",
       "      <td>0.427607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>378</td>\n",
       "      <td>0.522333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>379</td>\n",
       "      <td>0.645030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>380</td>\n",
       "      <td>0.801018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>381</td>\n",
       "      <td>0.914964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>382</td>\n",
       "      <td>0.910845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>383</td>\n",
       "      <td>0.849068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>384</td>\n",
       "      <td>0.772189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>553</td>\n",
       "      <td>0.396613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>554</td>\n",
       "      <td>0.400379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>555</td>\n",
       "      <td>0.422980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>556</td>\n",
       "      <td>0.407285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>557</td>\n",
       "      <td>0.372129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>558</td>\n",
       "      <td>0.410424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>559</td>\n",
       "      <td>0.762613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>560</td>\n",
       "      <td>0.915793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>561</td>\n",
       "      <td>0.661539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>562</td>\n",
       "      <td>0.421096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>563</td>\n",
       "      <td>0.333206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>564</td>\n",
       "      <td>0.316883</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    row_id  meter_reading\n",
       "0      169       0.347524\n",
       "1      170       0.364863\n",
       "2      171       0.374867\n",
       "3      172       0.371532\n",
       "4      173       0.355526\n",
       "5      174       0.390872\n",
       "6      175       0.541342\n",
       "7      176       0.753001\n",
       "8      177       0.785012\n",
       "9      178       0.568936\n",
       "10     179       0.468233\n",
       "11     180       0.426885\n",
       "12     181       0.407545\n",
       "13     182       0.408212\n",
       "14     183       0.415548\n",
       "15     184       0.417548\n",
       "16     185       0.442891\n",
       "17     186       0.509581\n",
       "18     187       0.625622\n",
       "19     188       0.801018\n",
       "20     189       0.926396\n",
       "21     190       0.931731\n",
       "22     191       0.819691\n",
       "23     192       0.659634\n",
       "24     361       0.793468\n",
       "25     362       0.699429\n",
       "26     363       0.668540\n",
       "27     364       0.623462\n",
       "28     365       0.623917\n",
       "29     366       0.669226\n",
       "30     367       0.787977\n",
       "31     368       0.860050\n",
       "32     369       0.727572\n",
       "33     370       0.598697\n",
       "34     371       0.500368\n",
       "35     372       0.433099\n",
       "36     373       0.411133\n",
       "37     374       0.396032\n",
       "38     375       0.380931\n",
       "39     376       0.402896\n",
       "40     377       0.427607\n",
       "41     378       0.522333\n",
       "42     379       0.645030\n",
       "43     380       0.801018\n",
       "44     381       0.914964\n",
       "45     382       0.910845\n",
       "46     383       0.849068\n",
       "47     384       0.772189\n",
       "48     553       0.396613\n",
       "49     554       0.400379\n",
       "50     555       0.422980\n",
       "51     556       0.407285\n",
       "52     557       0.372129\n",
       "53     558       0.410424\n",
       "54     559       0.762613\n",
       "55     560       0.915793\n",
       "56     561       0.661539\n",
       "57     562       0.421096\n",
       "58     563       0.333206\n",
       "59     564       0.316883"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Cell 6 (Path A): Inference & Submission (model-side instance norm only) ---\n",
    "\n",
    "import torch, numpy as np, pandas as pd, inspect\n",
    "\n",
    "forecast_model.eval()\n",
    "device = next(forecast_model.parameters()).device\n",
    "\n",
    "# 0) Model context length & horizon\n",
    "SL = int(getattr(forecast_model.config, \"context_length\", 512))   # ckpt CL (e.g., 512)\n",
    "H  = int(getattr(forecast_model.config, \"prediction_length\", 24)) # pruned head to this\n",
    "print(f\"[model] context_length={SL}, prediction_length={H}\")\n",
    "\n",
    "# 1) Build window_id -> ordered row_id list (TARGET rows from original test)\n",
    "assert {\"window_id\",\"timestamp\",\"row_id\",\"role\"}.issubset(test_df1.columns), \\\n",
    "    \"test_df1 must contain window_id, timestamp, row_id, role\"\n",
    "\n",
    "target_rows = (test_df1.loc[test_df1[\"role\"] == \"target\", [\"window_id\",\"timestamp\",\"row_id\"]]\n",
    "               .copy()\n",
    "               .sort_values([\"window_id\",\"timestamp\"]))\n",
    "target_rows[\"window_id\"] = target_rows[\"window_id\"].astype(int)\n",
    "window_to_rowids = {wid: grp[\"row_id\"].tolist()\n",
    "                    for wid, grp in target_rows.groupby(\"window_id\", sort=False)}\n",
    "\n",
    "# 2) helpers\n",
    "def _extract_forecast(outputs):\n",
    "    # Try common fields\n",
    "    if isinstance(outputs, dict):\n",
    "        for k in (\"forecast\",\"forecasts\",\"prediction_outputs\",\"predictions\",\"mean\",\"loc\"):\n",
    "            if k in outputs:\n",
    "                v = outputs[k]\n",
    "                return v[0] if isinstance(v, (list, tuple)) and torch.is_tensor(v[0]) else v\n",
    "    for k in (\"forecast\",\"forecasts\",\"prediction_outputs\",\"predictions\",\"mean\",\"loc\"):\n",
    "        if hasattr(outputs, k):\n",
    "            v = getattr(outputs, k)\n",
    "            return v[0] if isinstance(v, (list, tuple)) and torch.is_tensor(v[0]) else v\n",
    "    if isinstance(outputs, (list, tuple)) and torch.is_tensor(outputs[0]):\n",
    "        return outputs[0]\n",
    "    raise RuntimeError(\"Could not find forecast tensor in model output.\")\n",
    "\n",
    "def _extract_wid(sample):\n",
    "    if \"window_id\" in sample:\n",
    "        v = sample[\"window_id\"]\n",
    "        if torch.is_tensor(v): return int(v.view(-1)[0].item())\n",
    "        if isinstance(v, (np.integer, int, float)): return int(v)\n",
    "        if isinstance(v, (list, tuple)) and len(v):\n",
    "            vv = v[0]; return int(vv.item() if torch.is_tensor(vv) else vv)\n",
    "    if \"id\" in sample:\n",
    "        v = sample[\"id\"]\n",
    "        if torch.is_tensor(v): return int(v.view(-1)[0].item())\n",
    "        if isinstance(v, (np.integer, int, float)): return int(v)\n",
    "        if isinstance(v, (list, tuple)) and len(v):\n",
    "            for vv in v:\n",
    "                if torch.is_tensor(vv): return int(vv.view(-1)[0].item())\n",
    "                if isinstance(vv, (np.integer, int, float)): return int(vv)\n",
    "            try: return int(v[-1])\n",
    "            except: pass\n",
    "        if isinstance(v, dict):\n",
    "            if \"window_id\" in v: return int(v[\"window_id\"])\n",
    "            for vv in v.values():\n",
    "                if isinstance(vv, (np.integer, int, float)): return int(vv)\n",
    "    raise KeyError(\"Cannot extract window_id from sample.\")\n",
    "\n",
    "def _pad_or_crop_to_SL(pv: torch.Tensor, pom: torch.Tensor, SL: int):\n",
    "    # pv, pom are [T, C]\n",
    "    T, C = pv.shape\n",
    "    if T == SL:\n",
    "        return pv, pom\n",
    "    if T > SL:\n",
    "        return pv[-SL:, :], pom[-SL:, :]\n",
    "    pad = SL - T\n",
    "    filler = pv.mean(dim=0) if T > 0 else torch.zeros(C, dtype=pv.dtype, device=pv.device)\n",
    "    pv2  = torch.cat([filler.expand(pad, C), pv], dim=0)         # [SL, C]\n",
    "    pom2 = torch.cat([torch.zeros(pad, C, dtype=pom.dtype, device=pom.device), pom], dim=0)\n",
    "    return pv2, pom2\n",
    "\n",
    "# 3) inference loop\n",
    "bs = 64\n",
    "N  = len(test_dataset)\n",
    "preds_per_window = {}\n",
    "\n",
    "forward_allowed = set(inspect.signature(forecast_model.forward).parameters.keys())\n",
    "FREQ_ID = 1  # hourly id (only used if model actually accepts freq_token)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in range(0, N, bs):\n",
    "        end   = min(N, start + bs)\n",
    "        batch = [test_dataset[i] for i in range(start, end)]\n",
    "        wids  = [ _extract_wid(s) for s in batch ]\n",
    "\n",
    "        # Prepare per-sample padded past_values / past_observed_mask\n",
    "        pv_list, pom_list = [], []\n",
    "        for s in batch:\n",
    "            pv  = s[\"past_values\"]\n",
    "            pom = s.get(\"past_observed_mask\", None)\n",
    "            pv  = torch.as_tensor(pv, device=device, dtype=torch.float32)        # [T, C]\n",
    "            if pom is None:\n",
    "                pom = torch.ones_like(pv, device=device)                         # assume fully observed\n",
    "            else:\n",
    "                pom = torch.as_tensor(pom, device=device, dtype=torch.float32)\n",
    "\n",
    "            pv, pom = _pad_or_crop_to_SL(pv, pom, SL)                            # mask zeros on pad\n",
    "            pv_list.append(pv)\n",
    "            pom_list.append(pom)\n",
    "\n",
    "        model_inputs = {\n",
    "            \"past_values\":        torch.stack(pv_list, 0),   # [B, SL, C]\n",
    "            \"past_observed_mask\": torch.stack(pom_list, 0),  # [B, SL, C]\n",
    "        }\n",
    "\n",
    "        # Optional features if present in the dataset and accepted by the model\n",
    "        for opt_key in (\"metadata\", \"static_categorical_values\"):\n",
    "            if (opt_key in batch[0]) and (opt_key in forward_allowed):\n",
    "                vals = [torch.as_tensor(s[opt_key]) if not torch.is_tensor(s[opt_key]) else s[opt_key] for s in batch]\n",
    "                model_inputs[opt_key] = torch.stack(vals, 0).to(device)\n",
    "\n",
    "        # If the model accepts freq_token, provide a constant hourly token\n",
    "        if \"freq_token\" in forward_allowed:\n",
    "            model_inputs[\"freq_token\"] = torch.full((len(batch),), FREQ_ID, dtype=torch.long, device=device)\n",
    "\n",
    "        # IMPORTANT: don't pass future_* at test-time (no loss, just predictions)\n",
    "        outputs = forecast_model(**model_inputs, return_dict=True, return_loss=False)\n",
    "        yhat = _extract_forecast(outputs)  # e.g., [B, H] or [B, H, 1] or [B, H, C?]\n",
    "\n",
    "        # Normalize shapes to [B, H]\n",
    "        if yhat.dim() == 3 and yhat.size(-1) == 1:\n",
    "            yhat = yhat.squeeze(-1)\n",
    "        elif yhat.dim() == 3 and yhat.size(-1) > 1:\n",
    "            # pick target channel index 0 (or use tsp.prediction_channel_indices[0] if available)\n",
    "            tgt_idx = getattr(tsp, \"prediction_channel_indices\", [0])[0]\n",
    "            yhat = yhat[..., tgt_idx]\n",
    "        assert yhat.dim() == 2 and yhat.size(1) == H, f\"Expected [B, {H}], got {tuple(yhat.shape)}\"\n",
    "        yhat = yhat.detach().cpu().numpy()\n",
    "\n",
    "        # Path A: DO NOT inverse-transform with tsp/global scalers.\n",
    "        for wid, seq in zip(wids, yhat):\n",
    "            seq_unscaled = seq                          # model did per-window norm internally\n",
    "            seq_unscaled = np.clip(seq_unscaled, 0, None)\n",
    "            preds_per_window[wid] = seq_unscaled\n",
    "\n",
    "# 4) assemble submission\n",
    "rows = []\n",
    "missing_windows = []\n",
    "count_mismatch  = 0\n",
    "\n",
    "for wid, pred in preds_per_window.items():\n",
    "    row_ids = window_to_rowids.get(wid)\n",
    "    if not row_ids:\n",
    "        missing_windows.append(wid)\n",
    "        continue\n",
    "    if len(row_ids) != len(pred):\n",
    "        count_mismatch += 1\n",
    "    m = min(len(row_ids), len(pred))\n",
    "    rows.extend(zip(row_ids[:m], pred[:m]))\n",
    "\n",
    "if missing_windows:\n",
    "    print(f\"[warn] {len(missing_windows)} predicted windows missing row_ids (e.g., {missing_windows[:5]})\")\n",
    "if count_mismatch:\n",
    "    print(f\"[warn] {count_mismatch} windows had horizon length mismatch; truncated.\")\n",
    "\n",
    "submission = pd.DataFrame(rows, columns=[\"row_id\", \"meter_reading\"]).sort_values(\"row_id\")\n",
    "print(\"Submission shape:\", submission.shape)\n",
    "display(submission.head(60))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47d1c445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote submission_seed_79_r2.csv\n"
     ]
    }
   ],
   "source": [
    "# Save\n",
    "submission_path = f\"submission_seed_{SEED}_r2.csv\"\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"✅ Wrote {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9325ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Cell B (MSE-only, no calibration): convex blend seeds and write final submission ---\n",
    "\n",
    "# import numpy as np, pandas as pd\n",
    "\n",
    "# # point to your per-seed validation preds and ground truth (from Cell A)\n",
    "# VAL_FILES = [\n",
    "#     \"val_preds_seed40_r1.csv\",  # id,y_hat\n",
    "#     \"val_preds_seed42_r1.csv\",\n",
    "#     \"val_preds_seed50_r1.csv\",\n",
    "# ]\n",
    "# VAL_GT = \"val_ground_truth.csv\"       # id,y_true\n",
    "\n",
    "# # point to your per-seed test submissions\n",
    "# TEST_FILES = [\n",
    "#     \"submission_seed_40_r1.csv\",       # row_id,meter_reading\n",
    "#     \"submission_seed_42_r1.csv\",\n",
    "#     \"submission_seed_50_r1.csv\",\n",
    "# ]\n",
    "\n",
    "# def _project_to_simplex(v):\n",
    "#     v = np.asarray(v, dtype=np.float64)\n",
    "#     if np.isclose(v.sum(), 1.0) and np.all(v >= 0): return v\n",
    "#     n = v.size; u = np.sort(v)[::-1]; cssv = np.cumsum(u)\n",
    "#     rho = np.nonzero(u * np.arange(1, n+1) > (cssv - 1))[0][-1]\n",
    "#     theta = (cssv[rho] - 1) / float(rho + 1)\n",
    "#     return np.maximum(v - theta, 0.0)\n",
    "\n",
    "# def fit_convex_blend_weights_mse(val_preds_list, y_true, l2=1e-8, max_iter=2000, tol=1e-9):\n",
    "#     \"\"\"\n",
    "#     Solve: min_w  (1/N)||P w - y||^2 + l2||w||^2  s.t. w>=0, sum w = 1\n",
    "#     \"\"\"\n",
    "#     P = np.stack([p.astype(np.float64) for p in val_preds_list], axis=1)  # [N, K]\n",
    "#     y = y_true.astype(np.float64)\n",
    "#     N, K = P.shape\n",
    "#     # Lipschitz step for PGD\n",
    "#     M = (P.T @ P) / N + l2 * np.eye(K)\n",
    "#     L = float(np.linalg.eigvalsh(M).max())\n",
    "#     step = 1.0 / (L + 1e-12)\n",
    "\n",
    "#     w = np.ones(K) / K\n",
    "#     for _ in range(max_iter):\n",
    "#         grad = (P.T @ (P @ w - y)) / N + l2 * w\n",
    "#         w_new = _project_to_simplex(w - step * grad)\n",
    "#         if np.linalg.norm(w_new - w) < tol:\n",
    "#             w = w_new\n",
    "#             break\n",
    "#         w = w_new\n",
    "#     blended_val = P @ w\n",
    "#     return w, blended_val\n",
    "\n",
    "# def mse(a, b):\n",
    "#     a = np.asarray(a, np.float64); b = np.asarray(b, np.float64)\n",
    "#     return float(np.mean((a - b) ** 2))\n",
    "\n",
    "# # 1) load validation ground truth and merge per-seed val preds\n",
    "# gt = pd.read_csv(VAL_GT)              # columns: id, y_true\n",
    "# base = gt.copy()\n",
    "# val_cols = []\n",
    "# for i, f in enumerate(VAL_FILES):\n",
    "#     df = pd.read_csv(f)               # columns: id, y_hat\n",
    "#     col = f\"y_hat_{i}\"\n",
    "#     df = df.rename(columns={\"y_hat\": col}) if \"y_hat\" in df.columns else df\n",
    "#     base = base.merge(df[[\"id\", col]], on=\"id\", how=\"inner\")\n",
    "#     val_cols.append(col)\n",
    "\n",
    "# y_true = base[\"y_true\"].to_numpy()\n",
    "# val_preds_list = [base[c].to_numpy() for c in val_cols]\n",
    "\n",
    "# # 2) fit convex MSE weights and print diagnostics\n",
    "# w, blended_val = fit_convex_blend_weights_mse(val_preds_list, y_true, l2=1e-8)\n",
    "# print(\"\\n=== Convex blend weights (sum=1, MSE) ===\")\n",
    "# for f, wi in zip(VAL_FILES, w):\n",
    "#     print(f\"  {f}: {wi:.4f}\")\n",
    "# print(f\"Val MSE (best single): {min(mse(p, y_true) for p in val_preds_list):.6f}\")\n",
    "# print(f\"Val MSE (blended)    : {mse(blended_val, y_true):.6f}\")\n",
    "\n",
    "# # 3) apply the same weights to test submissions, save final\n",
    "# subs = [pd.read_csv(f)[[\"row_id\",\"meter_reading\"]].sort_values(\"row_id\").reset_index(drop=True)\n",
    "#         for f in TEST_FILES]\n",
    "# row_ids = subs[0][\"row_id\"].to_numpy()\n",
    "# for i, df in enumerate(subs[1:], start=2):\n",
    "#     if not np.array_equal(row_ids, df[\"row_id\"].to_numpy()):\n",
    "#         raise ValueError(f\"Row IDs differ between test file 1 and test file {i}.\")\n",
    "\n",
    "# stack = np.stack([df[\"meter_reading\"].to_numpy() for df in subs], axis=1)  # [N, K]\n",
    "# y_blend_test = stack @ w\n",
    "# y_blend_test = np.clip(y_blend_test, 0.0, None)\n",
    "\n",
    "# final = pd.DataFrame({\"row_id\": row_ids, \"meter_reading\": y_blend_test.astype(np.float32)})\n",
    "# final.to_csv(\"submission_blend_mse_only_r1.csv\", index=False)\n",
    "# print(\"✅ Wrote submission_blend_mse_only_r1.csv | rows =\", len(final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f18e55ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[R1] per-seed weights: 0.2855 0.2724 0.1944 0.0711 0.1766 | best single MSE: 0.599243 | blended MSE: 0.599190\n",
      "[R2] per-seed weights: 0.2572 0.0002 0.2248 0.1263 0.3914 | best single MSE: 0.605412 | blended MSE: 0.603942\n",
      "\n",
      "[Family→Scalar] alpha(R1)=0.6731, (1-alpha)(R2)=0.3269 | Val MSE=0.597725\n",
      "✅ Wrote submission_blend_r1r2_family_then_scalar_5seeds.csv | rows=84600\n",
      "\n",
      "[Global-6] weights: 0.1748 0.1703 0.1256 0.0770 0.1312 0.0673 0.0000 0.0739 0.0000 0.1799 | best single MSE: 0.599243 | blended MSE: 0.597624\n",
      "✅ Wrote submission_blend_r1r2_global10_5seeds.csv | rows=84600\n"
     ]
    }
   ],
   "source": [
    "# --- Blend R1-ensemble with R2-ensemble (MSE-only, no calibration) ---\n",
    "# It (1) fits convex weights per family (R1, R2) on validation,\n",
    "#     (2) fits a scalar α to blend the two family ensembles on validation,\n",
    "#     (3) applies those weights to test CSVs and writes the final submission.\n",
    "#\n",
    "# Also includes Option B: one-shot convex blend across ALL 6 seeds.\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# =========== EDIT ONLY THESE PATHS ============\n",
    "\n",
    "# Validation ground truth (from your Cell A)\n",
    "VAL_GT = \"val_ground_truth.csv\"     # columns: id,y_true\n",
    "\n",
    "# R1 validation prediction files (id,y_hat) and test submissions (row_id,meter_reading)\n",
    "R1_VAL_FILES = [\n",
    "    \"val_preds_seed40_r1.csv\",\n",
    "    \"val_preds_seed42_r1.csv\",\n",
    "    \"val_preds_seed50_r1.csv\",\n",
    "    \"val_preds_seed79_r1.csv\",\n",
    "    \"val_preds_seed17_r1.csv\"\n",
    "]\n",
    "R1_TEST_FILES = [\n",
    "    \"submission_seed_40_r1.csv\",\n",
    "    \"submission_seed_42_r1.csv\",\n",
    "    \"submission_seed_50_r1.csv\",\n",
    "    \"submission_seed_79_r1.csv\",\n",
    "    \"submission_seed_17_r1.csv\"\n",
    "]\n",
    "\n",
    "# R2 validation prediction files (id,y_hat) and test submissions (row_id,meter_reading)\n",
    "R2_VAL_FILES = [\n",
    "    \"val_preds_seed40.csv\",\n",
    "    \"val_preds_seed42.csv\",\n",
    "    \"val_preds_seed50.csv\",\n",
    "    \"val_preds_seed79_r2.csv\",\n",
    "    \"val_preds_seed19_r2.csv\"\n",
    "]\n",
    "R2_TEST_FILES = [\n",
    "    \"submission_seed_40_0002.csv\",\n",
    "    \"submission_seed_42_0002.csv\",\n",
    "    \"submission_seed_50_0002.csv\",\n",
    "    \"submission_seed_79_r2.csv\",\n",
    "    \"submission_seed_19_r2.csv\"\n",
    "]\n",
    "\n",
    "OUT_FAMILY_BLEND = \"submission_blend_r1r2_family_then_scalar_5seeds.csv\"\n",
    "OUT_GLOBAL_BLEND = \"submission_blend_r1r2_global10_5seeds.csv\"\n",
    "\n",
    "# ==============================================\n",
    "\n",
    "def _project_to_simplex(v):\n",
    "    v = np.asarray(v, dtype=np.float64)\n",
    "    if np.isclose(v.sum(), 1.0) and np.all(v >= 0): return v\n",
    "    n = v.size; u = np.sort(v)[::-1]; cssv = np.cumsum(u)\n",
    "    rho = np.nonzero(u * np.arange(1, n+1) > (cssv - 1))[0][-1]\n",
    "    theta = (cssv[rho] - 1) / float(rho + 1)\n",
    "    return np.maximum(v - theta, 0.0)\n",
    "\n",
    "def fit_convex_blend_weights_mse(val_preds_list, y_true, l2=1e-8, max_iter=2000, tol=1e-9):\n",
    "    \"\"\"\n",
    "    Solve: min_w (1/N)||P w - y||^2 + l2||w||^2  s.t. w>=0, sum w = 1\n",
    "    Returns weights w and blended validation preds P@w\n",
    "    \"\"\"\n",
    "    P = np.stack([p.astype(np.float64) for p in val_preds_list], axis=1)  # [N,K]\n",
    "    y = y_true.astype(np.float64)\n",
    "    N, K = P.shape\n",
    "    # Lipschitz step for PGD\n",
    "    M = (P.T @ P) / N + l2 * np.eye(K)\n",
    "    L = float(np.linalg.eigvalsh(M).max())\n",
    "    step = 1.0 / (L + 1e-12)\n",
    "\n",
    "    w = np.ones(K) / K\n",
    "    for _ in range(max_iter):\n",
    "        grad = (P.T @ (P @ w - y)) / N + l2 * w\n",
    "        w_new = _project_to_simplex(w - step * grad)\n",
    "        if np.linalg.norm(w_new - w) < tol:\n",
    "            w = w_new\n",
    "            break\n",
    "        w = w_new\n",
    "    return w, (P @ w)\n",
    "\n",
    "def mse(a, b):\n",
    "    a = np.asarray(a, np.float64); b = np.asarray(b, np.float64)\n",
    "    return float(np.mean((a - b) ** 2))\n",
    "\n",
    "def _load_val_matrix(files, base_ids):\n",
    "    \"\"\"Merge multiple 'id,y_hat' CSVs onto base id order; return stacked preds [N,K].\"\"\"\n",
    "    mats = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        # Normalize column names\n",
    "        if \"y_hat\" not in df.columns and \"meter_reading\" in df.columns:\n",
    "            df = df.rename(columns={\"meter_reading\":\"y_hat\"})\n",
    "        df = df[[\"id\",\"y_hat\"]].drop_duplicates(\"id\", keep=\"last\")\n",
    "        # align on base ids\n",
    "        df = base_ids.merge(df, on=\"id\", how=\"inner\")\n",
    "        mats.append(df[\"y_hat\"].to_numpy())\n",
    "    return np.stack(mats, axis=1)  # [N,K]\n",
    "\n",
    "def _blend_test_from_weights(test_files, w):\n",
    "    \"\"\"Apply weights w to list of test CSVs (row_id,meter_reading).\"\"\"\n",
    "    subs = [pd.read_csv(f)[[\"row_id\",\"meter_reading\"]].sort_values(\"row_id\").reset_index(drop=True)\n",
    "            for f in test_files]\n",
    "    row_ids = subs[0][\"row_id\"].to_numpy()\n",
    "    for i, df in enumerate(subs[1:], start=2):\n",
    "        if not np.array_equal(row_ids, df[\"row_id\"].to_numpy()):\n",
    "            raise ValueError(f\"Row IDs differ between test file 1 and test file {i}.\")\n",
    "    stack = np.stack([df[\"meter_reading\"].to_numpy() for df in subs], axis=1)  # [N,K]\n",
    "    y = stack @ w\n",
    "    return row_ids, np.clip(y, 0.0, None)\n",
    "\n",
    "# ---------- 1) Load validation ground truth ----------\n",
    "gt = pd.read_csv(VAL_GT)[[\"id\",\"y_true\"]].drop_duplicates(\"id\", keep=\"last\").sort_values(\"id\").reset_index(drop=True)\n",
    "base_ids = gt[[\"id\"]]  # keep order\n",
    "y_true = gt[\"y_true\"].to_numpy()\n",
    "\n",
    "# ---------- 2) Family-level convex weights on validation ----------\n",
    "# R1 family\n",
    "P_r1 = _load_val_matrix(R1_VAL_FILES, base_ids)  # [N, K1]\n",
    "w_r1, r1_val_blend = fit_convex_blend_weights_mse([P_r1[:,i] for i in range(P_r1.shape[1])], y_true, l2=1e-8)\n",
    "print(\"\\n[R1] per-seed weights:\", \" \".join(f\"{wi:.4f}\" for wi in w_r1),\n",
    "      \"| best single MSE:\", f\"{min(mse(P_r1[:,i], y_true) for i in range(P_r1.shape[1])):.6f}\",\n",
    "      \"| blended MSE:\", f\"{mse(r1_val_blend, y_true):.6f}\")\n",
    "\n",
    "# R2 family\n",
    "P_r2 = _load_val_matrix(R2_VAL_FILES, base_ids)  # [N, K2]\n",
    "w_r2, r2_val_blend = fit_convex_blend_weights_mse([P_r2[:,i] for i in range(P_r2.shape[1])], y_true, l2=1e-8)\n",
    "print(\"[R2] per-seed weights:\", \" \".join(f\"{wi:.4f}\" for wi in w_r2),\n",
    "      \"| best single MSE:\", f\"{min(mse(P_r2[:,i], y_true) for i in range(P_r2.shape[1])):.6f}\",\n",
    "      \"| blended MSE:\", f\"{mse(r2_val_blend, y_true):.6f}\")\n",
    "\n",
    "# ---------- 3) Learn scalar α to blend R1 vs R2 on validation ----------\n",
    "# We solve min_{α∈[0,1]} MSE( α r1 + (1-α) r2, y_true )\n",
    "# This is a convex 1D problem; closed form under no box, then clamp to [0,1].\n",
    "r1 = r1_val_blend.astype(np.float64)\n",
    "r2 = r2_val_blend.astype(np.float64)\n",
    "y  = y_true.astype(np.float64)\n",
    "\n",
    "num = np.sum((r1 - r2) * (y - r2))\n",
    "den = np.sum((r1 - r2) ** 2) + 1e-12\n",
    "alpha = float(np.clip(num / den, 0.0, 1.0))  # α on R1; (1-α) on R2\n",
    "val_mse_family_scalar = mse(alpha * r1 + (1 - alpha) * r2, y)\n",
    "print(f\"\\n[Family→Scalar] alpha(R1)={alpha:.4f}, (1-alpha)(R2)={(1-alpha):.4f} | Val MSE={val_mse_family_scalar:.6f}\")\n",
    "\n",
    "# ---------- 4) Apply to TEST: family blends then scalar blend ----------\n",
    "row_ids_r1, y_r1_test = _blend_test_from_weights(R1_TEST_FILES, w_r1)\n",
    "row_ids_r2, y_r2_test = _blend_test_from_weights(R2_TEST_FILES, w_r2)\n",
    "if not np.array_equal(row_ids_r1, row_ids_r2):\n",
    "    raise ValueError(\"Row IDs differ between R1 and R2 test blends.\")\n",
    "y_final_family = alpha * y_r1_test + (1 - alpha) * y_r2_test\n",
    "final_family = pd.DataFrame({\"row_id\": row_ids_r1, \"meter_reading\": y_final_family.astype(np.float32)})\n",
    "final_family.to_csv(OUT_FAMILY_BLEND, index=False)\n",
    "print(f\"✅ Wrote {OUT_FAMILY_BLEND} | rows={len(final_family)}\")\n",
    "\n",
    "# ---------- (Optional) 5) One-shot global convex blend across ALL 6 seeds ----------\n",
    "# Build global validation matrix [N, K1+K2] and learn weights; apply to all 6 test files.\n",
    "ALL_VAL_FILES = R1_VAL_FILES + R2_VAL_FILES\n",
    "ALL_TEST_FILES = R1_TEST_FILES + R2_TEST_FILES\n",
    "\n",
    "P_all = _load_val_matrix(ALL_VAL_FILES, base_ids)  # [N, K]\n",
    "w_all, val_blend_all = fit_convex_blend_weights_mse([P_all[:,i] for i in range(P_all.shape[1])], y_true, l2=1e-8)\n",
    "print(\"\\n[Global-6] weights:\", \" \".join(f\"{wi:.4f}\" for wi in w_all),\n",
    "      \"| best single MSE:\", f\"{min(mse(P_all[:,i], y_true) for i in range(P_all.shape[1])):.6f}\",\n",
    "      \"| blended MSE:\", f\"{mse(val_blend_all, y_true):.6f}\")\n",
    "\n",
    "# Apply to test\n",
    "subs_all = [pd.read_csv(f)[[\"row_id\",\"meter_reading\"]].sort_values(\"row_id\").reset_index(drop=True)\n",
    "            for f in ALL_TEST_FILES]\n",
    "row_ids_all = subs_all[0][\"row_id\"].to_numpy()\n",
    "for i, df in enumerate(subs_all[1:], start=2):\n",
    "    if not np.array_equal(row_ids_all, df[\"row_id\"].to_numpy()):\n",
    "        raise ValueError(f\"Row IDs differ between global test file 1 and {i}.\")\n",
    "stack_all = np.stack([df[\"meter_reading\"].to_numpy() for df in subs_all], axis=1)  # [N,K]\n",
    "y_global = stack_all @ w_all\n",
    "y_global = np.clip(y_global, 0.0, None)\n",
    "final_global = pd.DataFrame({\"row_id\": row_ids_all, \"meter_reading\": y_global.astype(np.float32)})\n",
    "final_global.to_csv(OUT_GLOBAL_BLEND, index=False)\n",
    "print(f\"✅ Wrote {OUT_GLOBAL_BLEND} | rows={len(final_global)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsfm_suyog_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
